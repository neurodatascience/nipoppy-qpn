{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "from dateutil import relativedelta\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API call for redcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_call(url, query, logger=None):\n",
    "    \"\"\" helper function to make API calls to RedCap\n",
    "    \"\"\"\n",
    "    r = requests.post(url, data=query, verify=False)\n",
    "    http_status = str(r.status_code)\n",
    "    print(f'HTTP Status: {http_status}')\n",
    "\n",
    "    if http_status == \"200\":\n",
    "        query_results = r.json()\n",
    "        query_df = pd.DataFrame(query_results)\n",
    "\n",
    "    else:\n",
    "        print(f\"RedCap API request Failed with HTTP Status: {http_status}\")\n",
    "        query_df = None\n",
    "        \n",
    "    return query_df\n",
    "\n",
    "def get_inventory_count(df, index_col, availability_indicators):\n",
    "    \"\"\" helper function to count participants with recorded data in redcap\n",
    "    \"\"\"\n",
    "    assess_cols = df.columns.drop(index_col)\n",
    "\n",
    "    if availability_indicators == 'number':\n",
    "        df = df.replace(\"\", np.nan)\n",
    "        df[assess_cols] = df[assess_cols].astype(np.float64)\n",
    "\n",
    "    inventory = {}\n",
    "    for col in assess_cols:        \n",
    "        if availability_indicators == 'number':\n",
    "            availability_count = df[~df[col].isna()][index_col].nunique()\n",
    "        else:\n",
    "            availability_count = df[df[col].isin(availability_indicators)][index_col].nunique()\n",
    "        inventory[col] = availability_count\n",
    "    return inventory\n",
    "\n",
    "def get_available_data(config_json, DATASET_ROOT, var_name, preferred_var_source=\"primary\"):\n",
    "    \"\"\" Get data for given variables from available sources\n",
    "        All return dataframes should have participant_id and visit_id as index\n",
    "    \"\"\"\n",
    "    config_data = json.load(open(config_json))\n",
    "    data_sources = config_data['data_sources']\n",
    "    variable_info = config_data['variables'][var_name]\n",
    "    variable_type = variable_info[\"type\"]\n",
    "    variable_sources = variable_info[\"sources\"]\n",
    "\n",
    "    if preferred_var_source == \"primary\":\n",
    "        selected_var_source = variable_info['primary_source']\n",
    "        selected_var_instrument = variable_info['primary_instrument']\n",
    "    elif preferred_var_source == \"secondary\":\n",
    "        selected_var_source = variable_info['secondary_source']\n",
    "        selected_var_instrument = variable_info['secondary_instrument']\n",
    "    else:\n",
    "        print(f\"Using preferred source {preferred_var_source} for variable {var_name}\")\n",
    "        preferred_var_data_source = preferred_var_source[\"data_source\"]\n",
    "        preferred_var_instrument = preferred_var_source[\"instrument\"]\n",
    "\n",
    "        if preferred_var_data_source not in variable_sources.keys():\n",
    "            print(f\"Preferred data source {preferred_var_data_source} not available for variable {var_name}\")\n",
    "            return None\n",
    "        else:\n",
    "            selected_var_source = preferred_var_data_source\n",
    "\n",
    "        if preferred_var_instrument not in variable_sources[selected_var_source].keys():\n",
    "            print(f\"Preferred var instrument {preferred_var_instrument} not available for variable {var_name}\")\n",
    "            return None\n",
    "        else:\n",
    "            selected_var_instrument = preferred_var_instrument\n",
    "\n",
    "    print(f\"Using variable {var_name} from source {selected_var_source} and instrument {selected_var_instrument}\")\n",
    "\n",
    "    external_var_cols = variable_sources[selected_var_source][selected_var_instrument]\n",
    "\n",
    "    # Get data from primary source\n",
    "    var_file = data_sources[selected_var_source][selected_var_instrument][\"path\"]\n",
    "    var_file_path = f\"{DATASET_ROOT}/{var_file}\"\n",
    "    var_file_index = data_sources[selected_var_source][selected_var_instrument][\"index_cols\"]\n",
    "\n",
    "    var_df = pd.read_csv(var_file_path)\n",
    "    selected_var_cols = list(set(var_file_index + external_var_cols))\n",
    "    var_df = var_df[selected_var_cols]\n",
    "    \n",
    "    if (variable_type == \"date\") & (len(external_var_cols) == 1):\n",
    "        var_df[external_var_cols[0]] = pd.to_datetime(var_df[external_var_cols[0]], errors=\"coerce\", dayfirst=False)\n",
    "\n",
    "    if (len(external_var_cols) == 1):\n",
    "        var_df = var_df.rename(columns={external_var_cols[0]:var_name})\n",
    "    \n",
    "    return var_df\n",
    "\n",
    "def get_age_at_visit(df, date_col, age_col, dob_col=\"dob\", rounding_digits=2, age_range=(0,100)):\n",
    "    \"\"\" Get age at visit. Expects column name to be: var_date \"\"\"\n",
    "    \n",
    "    df[age_col] = df[date_col] - df[dob_col]\n",
    "    df[age_col] = np.round(df[age_col].dt.days / 365.25, rounding_digits)\n",
    "\n",
    "    if (len(df[df[age_col] > 100]) | len(df[df[age_col] < 0])):\n",
    "        print(f\"Warning: Age values outside range {age_range} for variable {var}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/home/nikhil/projects/Parkinsons/qpn/\"\n",
    "\n",
    "# Current nipoppy release\n",
    "current_release = \"June_2024\"\n",
    "\n",
    "data_release_dir = f\"{DATASET_ROOT}/releases/{current_release}/\"\n",
    "tabular_data_release_dir = f\"{data_release_dir}/tabular/\"\n",
    "\n",
    "redcap_release_dir = f\"{data_release_dir}tabular/redcap/chunked/\"\n",
    "redcap_chunked_report_COPY = f\"{redcap_release_dir}/1. COPN-QPNDataMoCAUPDRSNeur_DATA_LABELS_2024-06-19_0910_copy.xlsx\"\n",
    "colleted_redcap_report_file = f\"{redcap_release_dir}/redcap_chunked_report.csv\"\n",
    "\n",
    "redcap_legacy_updrs_file = f\"{redcap_release_dir}/COPN-QPNMDSUPDRS_DATA_LABELS_2024-06-19_0945.xlsx\"\n",
    "redcap_legacy_moca_file = f\"{redcap_release_dir}/COPN-QPNMoCA_DATA_LABELS_2024-06-19_0938.xlsx\"\n",
    "\n",
    "filtered_legacy_updrs_file = f\"{redcap_release_dir}/legacy_updrs.csv\"\n",
    "collated_moca_file = f\"{redcap_release_dir}/redcap_and_legacy_moca.csv\"\n",
    "\n",
    "demo_config_json = \"../workflow/tabular/demographics.json\"\n",
    "pheno_config_json = \"../workflow/tabular/pheno.json\"\n",
    "\n",
    "# output files\n",
    "demographics_file = f\"{tabular_data_release_dir}/demographics.csv\"\n",
    "mri_session_date_file = f\"{tabular_data_release_dir}/mri_sessions.csv\"\n",
    "updrs_file = f\"{tabular_data_release_dir}/assessments/updrs.csv\"\n",
    "moca_file = f\"{tabular_data_release_dir}/assessments/moca.csv\"\n",
    "dx_file = f\"{tabular_data_release_dir}/assessments/diagnosis.csv\"\n",
    "neuropsych_file = f\"{tabular_data_release_dir}/assessments/neuropsych.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized index names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_event_name = \"Baseline (Arm 1: C-OPN)\"\n",
    "\n",
    "## redcap event name variations\n",
    "config_data = json.load(open(demo_config_json))\n",
    "data_sources = config_data['data_sources']\n",
    "redcap_data_sources = data_sources['redcap']\n",
    "\n",
    "redcap_field_name_map = {}\n",
    "\n",
    "for instrument in redcap_data_sources.keys():\n",
    "    index_cols = redcap_data_sources[instrument]['index_cols']\n",
    "    record_id = index_cols[0]\n",
    "    event_name = index_cols[1]\n",
    "\n",
    "    redcap_field_name_map[record_id] = \"participant_id\"\n",
    "    redcap_field_name_map[event_name] = \"redcap_event_name\"\n",
    "print(f\"redcap_field_name_map: {redcap_field_name_map}\")\n",
    "\n",
    "# legacy participant_id variations in DOB and BD_RPQ\n",
    "legacy_field_name_map = {}\n",
    "legacy_field_name_map['Record ID'] = \"participant_id\"\n",
    "legacy_field_name_map['Patient #'] = \"participant_id\"\n",
    "legacy_field_name_map['Name of visit (V01, V02, V03)'] = \"visit\"\n",
    "print(f\"legacy_field_name_map: {legacy_field_name_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update RedCAP reports through API \n",
    "(Not updating extended report since it has to come from Sarah)\n",
    "- \"global_records_query\"\n",
    "- \"QPN MoCA-UPDRS-Neuropsy data_Sarah\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_redcap_reports = False\n",
    "\n",
    "redcap_report_list = [\"global_records_query\", \"QPN MoCA-UPDRS-Neuropsy data_Sarah\"]\n",
    "if update_redcap_reports:\n",
    "    redcap_config_json = f\"{DATASET_ROOT}/proc/.redcap.json\"\n",
    "    redcap_config = json.load(open(redcap_config_json))\n",
    "    url = redcap_config[\"url\"]\n",
    "    \n",
    "    for redcap_report in redcap_report_list:\n",
    "        print(f\"Getting data for RedCap report: {redcap_report}\")\n",
    "        records_query = redcap_config[\"queries\"][redcap_report]\n",
    "        query_df = api_call(url, records_query, logger=None)\n",
    "        report_csv = f\"{tabular_data_release_dir}/redcap/{redcap_report}.csv\"\n",
    "        query_df.to_csv(report_csv, index=False)\n",
    "        print(f\"Saved RedCap report to {report_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QPN_participants_df = get_available_data(demo_config_json,data_release_dir,\"participant_id\")\n",
    "QPN_participants = QPN_participants_df[\"participant_id\"].unique()\n",
    "n_participants = len(QPN_participants)\n",
    "print(f\"Number of participants: {n_participants}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate chunked RedCap data\n",
    "- The new generate report is formatted as mutli-tab excel spreadsheet based on redcap-event. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate_collated_report = False\n",
    "\n",
    "if regenerate_collated_report:\n",
    "    sheet_names = [\"Baseline (without CHQ)\",\"F-U 12months & MNI\",\"F-U 18months & MNI\", \"F-U 24months & MNI\",\n",
    "                \"F-U 12months & PD, UDM\", \"F-U 18months & PD, UDM\", \"F-U 24months & PD, UDM\"]\n",
    "    redcap_chunked_report_df = pd.DataFrame()\n",
    "    for sheet_name in sheet_names:\n",
    "        _df = pd.read_excel(redcap_chunked_report_COPY, sheet_name=sheet_name, engine='openpyxl')\n",
    "        _df = _df[_df[\"Record ID:\"].isin(QPN_participants)]  \n",
    "        redcap_chunked_report_df = pd.concat([redcap_chunked_report_df, _df], axis=0)\n",
    "        print(f\"Sheet: {sheet_name} - Shape: {_df.shape}\")\n",
    "        print(f\"redcap_chunked_report_df - Shape: {redcap_chunked_report_df.shape}\")\n",
    "\n",
    "    print(f\"Saving collated redcap report to {redcap_release_dir}/redcap_chunked_report.csv\")\n",
    "    redcap_chunked_report_df.to_csv(collated_redcap_report_file, index=False)\n",
    "\n",
    "else:\n",
    "    print(f\"Loading collated redcap report from {redcap_release_dir}/redcap_chunked_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate and calculate legacy UPDRS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate_legacy_data = False\n",
    "\n",
    "if regenerate_legacy_data:\n",
    "    legacy_updrs_df = pd.read_excel(redcap_legacy_updrs_file, engine='openpyxl')\n",
    "\n",
    "    all_updrs3_cols = legacy_updrs_df.columns[legacy_updrs_df.columns.str.startswith(\"Updrs_3\")]\n",
    "    all_legacy_cols = legacy_updrs_df.columns[legacy_updrs_df.columns.str.endswith(\".1\")]\n",
    "\n",
    "    legacy_updrs3_cols = list(set(all_updrs3_cols) & set(all_legacy_cols))\n",
    "\n",
    "    legacy_total_cols = ['Part I: Non-Motor Aspects of Experiences of Daily Living (nM-EDL).1',\n",
    "                        'Part II: Motor Aspects of Experiences of Daily Living (M-EDL).1',\t\n",
    "                        'Part III: Motor Examination.1',\n",
    "                        'Part IV: Motor Complications.1']\n",
    "\n",
    "    legacy_admin_cols = ['Record ID:',\t'Event Name',\n",
    "                        'Assessment completed:     Évaluation remplie:  .1',\n",
    "                        'Assessment completed by:     Évaluation complétée par:.1',\n",
    "                        'How was the MDS-UPDRS administered?   Comment le MDS-UPDRS a-t-il été administré?.1']\n",
    "\n",
    "\n",
    "    legacy_filter_cols = legacy_admin_cols + legacy_total_cols + legacy_updrs3_cols\n",
    "\n",
    "    legacy_updrs_filtered_df = legacy_updrs_df.loc[:, legacy_filter_cols]\n",
    "\n",
    "    legacy_updrs_filtered_df = legacy_updrs_filtered_df.dropna(subset=legacy_updrs3_cols, how='all')\n",
    "\n",
    "    # Filter out two subjects that have all UPDRS subscore (most likely not legacy instrument)\n",
    "    legacy_updrs_filtered_df = legacy_updrs_filtered_df[legacy_updrs_filtered_df[\"Updrs_3_16_l value.1\"].isna()]\n",
    "\n",
    "    n_legacy_participants = legacy_updrs_filtered_df[\"Record ID:\"].nunique()\n",
    "\n",
    "    print(f\"Number of participants with legacy UPDRS data: {n_legacy_participants}\")\n",
    "\n",
    "    print(\"Summing all UPDRS3 sub-scores\")\n",
    "    legacy_updrs_filtered_df[\"legacy_updrs3\"] = legacy_updrs_filtered_df[legacy_updrs3_cols].sum(axis=1)\n",
    "    legacy_updrs_filtered_df[\"Event Name\"] = \"pre-redcap-baseline-1 (legacy)\"\n",
    "\n",
    "    print(f\"Saving filtered legacy UPDRS data to {filtered_legacy_updrs_file}\")\n",
    "    legacy_updrs_filtered_df.to_csv(filtered_legacy_updrs_file, index=False)\n",
    "\n",
    "    legacy_updrs_filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate and calculate legacy MoCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate_legacy_data = True\n",
    "\n",
    "if regenerate_legacy_data:\n",
    "    moca_df = pd.read_excel(redcap_legacy_moca_file, engine='openpyxl')\n",
    "\n",
    "    first_legacy_cols = moca_df.columns[moca_df.columns.str.endswith(\".1\")]\n",
    "    second_legacy_cols = moca_df.columns[moca_df.columns.str.endswith(\".2\")]\n",
    "\n",
    "    index_cols = ['Record ID:',\t'Event Name']\n",
    "    first_legacy_moca_df = moca_df.loc[:, index_cols + list(first_legacy_cols)]\n",
    "    second_legacy_moca_df = moca_df.loc[:, index_cols + list(second_legacy_cols)]\n",
    "\n",
    "    n_first_legacy_participants = first_legacy_moca_df[\"Record ID:\"].nunique()\n",
    "    n_second_legacy_participants = second_legacy_moca_df[\"Record ID:\"].nunique()\n",
    "\n",
    "    print(f\"Number of participants with first legacy MoCA data: {n_first_legacy_participants}\")\n",
    "    print(f\"Number of participants with second legacy MoCA data: {n_second_legacy_participants}\")\n",
    "\n",
    "    # merge first and second legacy moca data\n",
    "    moca_cols = first_legacy_cols.str.replace(\".1\",\"\")\n",
    "    first_legacy_cols_dict = dict(zip(first_legacy_cols, moca_cols))\n",
    "    second_legacy_cols_dict = dict(zip(second_legacy_cols, moca_cols))\n",
    "\n",
    "    first_legacy_moca_df = first_legacy_moca_df.rename(columns=first_legacy_cols_dict)\n",
    "    first_legacy_moca_df[\"Event Name\"] = \"pre-redcap-baseline-1 (legacy)\"\n",
    "\n",
    "    second_legacy_moca_df = second_legacy_moca_df.rename(columns=second_legacy_cols_dict)\n",
    "    second_legacy_moca_df[\"Event Name\"] = \"pre-redcap-baseline-2 (legacy)\"\n",
    "\n",
    "    legacy_moca_df = pd.concat([first_legacy_moca_df, second_legacy_moca_df], axis=0)\n",
    "\n",
    "    na_check_cols = legacy_moca_df.columns[legacy_moca_df.columns.str.startswith(\"TOTAL\")]\n",
    "\n",
    "    legacy_moca_df = legacy_moca_df.dropna(subset=na_check_cols, how='all')\n",
    "\n",
    "    n_legacy_participants = legacy_moca_df[\"Record ID:\"].nunique()\n",
    "    legacy_visit_counts = legacy_moca_df[\"Event Name\"].value_counts()\n",
    "\n",
    "    print(f\"Number of participants with legacy MoCA data: {n_legacy_participants}\")\n",
    "    print(f\"legacy_visit_counts MoCA data: {legacy_visit_counts}\")\n",
    "\n",
    "\n",
    "    # Merge legacy data with redcap visit data \n",
    "    print(\"-\"*50)\n",
    "    print(\"Merging redcap and legacy MoCA data\")\n",
    "    \n",
    "    redcap_moca_df = moca_df.loc[:, index_cols + list(moca_cols)]\n",
    "    n_redcap_participants = redcap_moca_df[\"Record ID:\"].nunique()\n",
    "    redcap_events = redcap_moca_df[\"Event Name\"].unique()\n",
    "    print(f\"Number of participants with redcap MoCA data: {n_redcap_participants}\")\n",
    "    print(f\"redcap_events MoCA data: {redcap_events}\")\n",
    "\n",
    "    redcap_and_legacy_moca_df = pd.concat([redcap_moca_df, legacy_moca_df], axis=0)\n",
    "    n_redcap_participants = redcap_and_legacy_moca_df[\"Record ID:\"].nunique()\n",
    "    redcap_events = redcap_and_legacy_moca_df[\"Event Name\"].unique()\n",
    "    print(f\"Number of participants with redcap and legacy MoCA data: {n_redcap_participants}\")\n",
    "    print(f\"redcap_events MoCA data: {redcap_events}\")\n",
    "\n",
    "    print(f\"Saving filtered legacy MoCA data to {collated_moca_file}\")\n",
    "    redcap_and_legacy_moca_df.to_csv(collated_moca_file, index=False)\n",
    "\n",
    "    legacy_moca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_vars = [\"dob\", \"group\", \"sex\", \"education\"]\n",
    "# preferred_var_source = {\"data_source\":\"local\",\"instrument\":\"legacy_DOB\"}\n",
    "vars_with_secondary_source = [\"dob\"]\n",
    "\n",
    "config_json = demo_config_json\n",
    "index_cols = [\"participant_id\", \"redcap_event_name\"]\n",
    "\n",
    "demo_var_df = pd.DataFrame()\n",
    "for var in demo_vars:\n",
    "    _df = get_available_data(config_json,data_release_dir,var)\n",
    "    _df = _df.rename(columns=redcap_field_name_map)\n",
    "    _df = _df.rename(columns=legacy_field_name_map)\n",
    "    _df = _df[_df[\"participant_id\"].isin(QPN_participants)].copy()\n",
    "\n",
    "    if var in vars_with_secondary_source:\n",
    "        print(f\"**Getting data from the secondary source for variable {var}**\")\n",
    "        _df2 = get_available_data(config_json,data_release_dir,var,preferred_var_source=\"secondary\")\n",
    "        _df2 = _df2.rename(columns=legacy_field_name_map)\n",
    "        _df2 = _df2.rename(columns={var:var+\"_secondary\"})\n",
    "        _df2 = _df2[_df2[\"participant_id\"].isin(QPN_participants)].copy()\n",
    "        \n",
    "        # Merge primary and secondary sources\n",
    "        n_missing_in_primary = _df[_df[\"redcap_event_name\"]==baseline_event_name][var].isna().sum()\n",
    "        print(f\"Missing data in primary source: {n_missing_in_primary}\")\n",
    "\n",
    "        if \"redcap_event_name\" in _df2.columns:\n",
    "            _df = pd.merge(_df, _df2, on=[\"participant_id\",\"redcap_event_name\"], how=\"outer\")\n",
    "        else:\n",
    "            _df = pd.merge(_df, _df2, on=\"participant_id\", how=\"outer\")\n",
    "        _df[var] = _df[var].fillna(_df[var+\"_secondary\"])\n",
    "        # _df = _df.drop(columns=[var+\"_secondary\"])\n",
    "\n",
    "        n_missing_after_secondary_fill = _df[_df[\"redcap_event_name\"]==baseline_event_name][var].isna().sum()\n",
    "        print(f\"Missing data after secondary source fill: {n_missing_after_secondary_fill}\")\n",
    "\n",
    "    if demo_var_df.empty:\n",
    "        demo_var_df = _df\n",
    "    else:\n",
    "        demo_var_df = pd.merge(demo_var_df, _df, on=index_cols, how=\"outer\")   \n",
    "\n",
    "demo_participants = demo_var_df[\"participant_id\"].unique()\n",
    "n_demo_participants = len(demo_participants)\n",
    "print('-'*50)\n",
    "print(f\"Number of participants with demographics data: {n_demo_participants}\")\n",
    "print('-'*50)\n",
    "\n",
    "demo_redcap_events = demo_var_df[\"redcap_event_name\"].unique()\n",
    "print(f\"Demographics data available for events: {demo_redcap_events}\")\n",
    "print('-'*50)\n",
    "\n",
    "for var in demo_vars:\n",
    "    n_unique = demo_var_df[demo_var_df[\"redcap_event_name\"]==\"Baseline (Arm 1: C-OPN)\"][var].nunique()\n",
    "    n_missing = demo_var_df[demo_var_df[\"redcap_event_name\"]==\"Baseline (Arm 1: C-OPN)\"][var].isna().sum()\n",
    "    print(f\"Var: {var}, n_unique: {n_unique}, n_missing: {n_missing} (out of {n_demo_participants})\")\n",
    "\n",
    "demo_var_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save demographics data **without the DOB** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_save_cols = [\"participant_id\", \"redcap_event_name\", \"group\", \"sex\", \"education\"]\n",
    "demo_var_without_dob_df = demo_var_df[demo_save_cols]\n",
    "demo_var_without_dob_df.to_csv(demographics_file, index=False)\n",
    "print(f\"Saved demographics data to {demographics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find records with phenotypic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_vars = [\"diagnosis\", \"updrs_scores\", \"legacy_updrs3_scores\", \"moca_scores\",\n",
    "              \"diagnosis_date\", \"updrs_date\", \"legacy_updrs3_date\", \"moca_date\"]\n",
    "\n",
    "config_json = pheno_config_json\n",
    "index_cols = [\"participant_id\", \"redcap_event_name\"]\n",
    "pheno_var_df = pd.DataFrame()\n",
    "for var in pheno_vars:\n",
    "    _df = get_available_data(config_json,data_release_dir,var)\n",
    "    _df = _df.rename(columns=redcap_field_name_map)\n",
    "    _df = _df.rename(columns=legacy_field_name_map)\n",
    "    _df = _df[_df[\"participant_id\"].isin(QPN_participants)].copy()\n",
    "    if pheno_var_df.empty:\n",
    "        pheno_var_df = _df\n",
    "    else:\n",
    "        pheno_var_df = pd.merge(pheno_var_df, _df, on=index_cols, how=\"outer\")   \n",
    "\n",
    "pheno_participants = pheno_var_df[\"participant_id\"].unique()\n",
    "n_pheno_participants = len(pheno_participants)\n",
    "print('-'*50)\n",
    "print(f\"Number of participants with pheno data: {n_pheno_participants}\")\n",
    "print('-'*50)\n",
    "\n",
    "pheno_redcap_events = pheno_var_df[\"redcap_event_name\"].unique()\n",
    "print(f\"Pheno data available for events: {pheno_redcap_events}\")\n",
    "print('-'*50)\n",
    "\n",
    "for var in pheno_var_df.columns:\n",
    "    for redcap_event in pheno_redcap_events:\n",
    "        if var not in index_cols:\n",
    "            pheno_var_event_df = pheno_var_df[pheno_var_df[\"redcap_event_name\"]==redcap_event].copy()\n",
    "            n_pheno_var_event_participants = pheno_var_event_df[\"participant_id\"].nunique()\n",
    "            if pheno_var_event_df[var].nunique() > 0:    \n",
    "                print(f\"Var: {var}, Event: {redcap_event}\")\n",
    "                n_unique = pheno_var_event_df[var].nunique()\n",
    "                n_missing = pheno_var_event_df[var].isna().sum()\n",
    "                print(f\"n_unique: {n_unique}, n_missing: {n_missing} (out of {n_pheno_var_event_participants})\")\n",
    "    print('-'*50)\n",
    "\n",
    "pheno_var_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuropsych data\n",
    "- Comes from either from Sarah's extended report or BD_RPQ_UPDATE_Neuropsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuropsych_vars = [\"neuropsy_scores\",\"neuropsy_date\"]\n",
    "\n",
    "config_data = json.load(open(config_json))\n",
    "variable_info = config_data['variables'][neuropsych_vars[0]]\n",
    "variable_sources = variable_info[\"sources\"]\n",
    "neuropsy_source = variable_info['primary_source']\n",
    "\n",
    "print(f\"Using neuropsych data source: {neuropsy_source}\")\n",
    "# local BD_RPQ data\n",
    "if neuropsy_source == \"local\":\n",
    "    index_cols = [\"participant_id\", \"visit\", \"TimePoint (based on REDCap; baseline, 18m, 36m, etc.)\", \"Délai depuis baseline (mois)\"]\n",
    "    \n",
    "# redcap data\n",
    "if neuropsy_source == \"redcap\":\n",
    "    index_cols = [\"participant_id\", \"redcap_event_name\"]\n",
    "\n",
    "    \n",
    "neuropsych_df = pd.DataFrame()\n",
    "for var in neuropsych_vars:\n",
    "    _df = get_available_data(config_json,data_release_dir,var)\n",
    "    _df = _df.rename(columns=redcap_field_name_map)\n",
    "    _df = _df.rename(columns=legacy_field_name_map)\n",
    "    _df = _df[_df[\"participant_id\"].isin(QPN_participants)].copy()\n",
    "    if neuropsych_df.empty:\n",
    "        neuropsych_df = _df\n",
    "    else:\n",
    "        neuropsych_df = pd.merge(neuropsych_df, _df, on=index_cols, how=\"left\")   \n",
    "\n",
    "neuropsych_participants = neuropsych_df[\"participant_id\"].unique()\n",
    "n_neuropsych_participants = len(neuropsych_participants)\n",
    "print('-'*50)\n",
    "print(f\"Number of participants with neuropysch data: {n_neuropsych_participants}\")\n",
    "print('-'*50)\n",
    "\n",
    "neuropsych_cols = neuropsych_df.columns.drop(index_cols).tolist()\n",
    "n_neuropsuch_cols = len(neuropsych_cols)\n",
    "print(f\"Neuropsych data available for variables: {n_neuropsuch_cols}\")\n",
    "print('-'*50)\n",
    "\n",
    "# BD_RPQ data\n",
    "if neuropsy_source == \"local\":\n",
    "    neuropsych_visits = neuropsych_df[\"visit\"].unique()\n",
    "\n",
    "# REDCap data\n",
    "if neuropsy_source == \"redcap\":\n",
    "    neuropsych_visits = neuropsych_df[\"redcap_event_name\"].unique()\n",
    "\n",
    "print(f\"neuropsych data available for events: {neuropsych_visits}\")\n",
    "print('-'*50)\n",
    "\n",
    "neuropsych_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic clean-up and data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix dtypes\n",
    "if neuropsy_source == \"local\":\n",
    "    for series_name, series in neuropsych_df.items():\n",
    "        if \"score\" in series_name:\n",
    "            if series.dtype == 'object':\n",
    "                print(f\"recasting {series_name} to float by replacing , with .\")\n",
    "                neuropsych_df[series_name] = neuropsych_df[series_name].str.replace(\",\",\".\").astype(float)\n",
    "                neuropsych_df.loc[neuropsych_df[series_name]>900, series_name] = np.nan\n",
    "                \n",
    "        # Replace >900 with NaNs\n",
    "        if series.dtype == 'float':\n",
    "            neuropsych_df.loc[neuropsych_df[series_name]>900, series_name] = np.nan\n",
    "\n",
    "    # assign redcap_event_name\n",
    "    visit_months = [12, 18, 24, 30, 36, 42, 48, 54]\n",
    "    month_bins = [9, 15, 21, 27, 33, 39, 45, 51, 57]\n",
    "    event_str_suffix = \"Months Follow-Up/Suivi (Arm 1: C-OPN)\"\n",
    "    event_names = [f\"{m} {event_str_suffix}\" for m in visit_months]\n",
    "\n",
    "    neuropsych_df[\"redcap_event_name\"] = pd.cut(neuropsych_df[\"Délai depuis baseline (mois)\"], bins=month_bins, labels=event_names).astype(str)\n",
    "    neuropsych_df.loc[neuropsych_df[\"TimePoint (based on REDCap; baseline, 18m, 36m, etc.)\"]==\"baseline\", \n",
    "                      \"redcap_event_name\"] = \"Baseline (Arm 1: C-OPN)\"\n",
    "\n",
    "# Merge with pheno_var_df\n",
    "index_cols = [\"participant_id\", \"redcap_event_name\"]\n",
    "pheno_var_df = pd.merge(pheno_var_df, neuropsych_df, on=index_cols, how=\"left\")  \n",
    "\n",
    "pheno_var_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add mri_acq date\n",
    "- Needs to map to redcap_event_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"MRI_date\"\n",
    "config_json = pheno_config_json\n",
    "mri_date_df = get_available_data(config_json,data_release_dir,var)\n",
    "mri_date_df[\"MRI_date\"] = pd.to_datetime(mri_date_df[\"MRI_date\"], errors=\"coerce\", dayfirst=False)\n",
    "\n",
    "n_mri_participants = mri_date_df[\"participant_id\"].nunique()\n",
    "print(f\"Number of participants with MRI data: {n_mri_participants}\")\n",
    "\n",
    "n_sessions = mri_date_df[\"session\"].nunique()\n",
    "print(f\"Number of MRI sessions: {n_sessions}\")\n",
    "\n",
    "participants_with_follow_ups = mri_date_df[mri_date_df[\"participant_id\"].duplicated()][\"participant_id\"].unique()\n",
    "n_participants_with_follow_ups = len(participants_with_follow_ups)\n",
    "print(f\"Number of participants with follow-up MRI: {n_participants_with_follow_ups}\")\n",
    "\n",
    "mri_ses01_date_df = mri_date_df[mri_date_df[\"session\"]==\"ses-01\"].copy()\n",
    "mri_ses01_date_df[\"redcap_event_name\"] = \"Baseline (Arm 1: C-OPN)\"\n",
    "\n",
    "mri_ses02_date_df = mri_date_df[mri_date_df[\"session\"]==\"ses-02\"].copy()\n",
    "mri_ses02_participants = mri_ses02_date_df[\"participant_id\"].unique()\n",
    "print(f\"Number of participants with ses-02 MRI: {len(mri_ses02_participants)}\")\n",
    "\n",
    "baseline_df = mri_ses01_date_df[mri_ses01_date_df[\"participant_id\"].isin(mri_ses02_participants)].set_index(\"participant_id\")\n",
    "followup_df = mri_ses02_date_df.set_index(\"participant_id\")\n",
    "\n",
    "visit_months = [12, 18, 24, 30, 36, 42, 48, 54]\n",
    "month_bins = [9, 15, 21, 27, 33, 39, 45, 51, 57]\n",
    "\n",
    "event_str_suffix = \"Months Follow-Up/Suivi (Arm 1: C-OPN)\"\n",
    "event_names = [f\"{m} {event_str_suffix}\" for m in visit_months]\n",
    "\n",
    "# --- Bin the months --- #\n",
    "followup_df[\"months_since_baseline\"] = followup_df[\"MRI_date\"].dt.to_period('M').astype(int) - baseline_df[\"MRI_date\"].dt.to_period('M').astype(int)\n",
    "followup_df[\"months_since_baseline\"] = followup_df[\"months_since_baseline\"].replace({0:np.nan}) # Some visits get same acq_date from brodacasting merge. \n",
    "\n",
    "followup_df[\"redcap_event_name\"] = pd.cut(followup_df[\"months_since_baseline\"], bins=month_bins, labels=event_names)\n",
    "\n",
    "mri_date_redcap_event_df = pd.concat([mri_ses01_date_df, followup_df.reset_index()], axis=0)\n",
    "# mri_date_redcap_event_df = mri_date_redcap_event_df\n",
    "\n",
    "mri_date_redcap_event_df.sort_values([\"participant_id\",\"session\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add MRI date to pheno data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_var_df[\"redcap_event_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_var_df = pd.merge(pheno_var_df, mri_date_redcap_event_df, on=index_cols, how=\"outer\")  \n",
    "var = \"MRI_date\"\n",
    "for redcap_event in mri_date_redcap_event_df[\"redcap_event_name\"].unique():    \n",
    "    pheno_var_event_df = pheno_var_df[pheno_var_df[\"redcap_event_name\"]==redcap_event].copy()\n",
    "    n_pheno_var_event_participants = pheno_var_event_df[\"participant_id\"].nunique()\n",
    "    if pheno_var_event_df[var].nunique() > 0:    \n",
    "        print(f\"Var: {var}, Event: {redcap_event}\")\n",
    "        n_unique = pheno_var_event_df[var].nunique()\n",
    "        n_missing = pheno_var_event_df[var].isna().sum()\n",
    "        print(f\"n_unique: {n_unique}, n_missing: {n_missing} (out of {n_pheno_var_event_participants})\")\n",
    "pheno_var_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_cols = [\"participant_id\", \"dob\", \"group\", \"sex\"]\n",
    "demo_var_df[demo_var_df[\"participant_id\"]==participants_with_follow_ups[0]]\n",
    "baseline_demo_df = demo_var_df[demo_var_df[\"redcap_event_name\"]==\"Baseline (Arm 1: C-OPN)\"][demo_cols].copy()\n",
    "\n",
    "index_cols = [\"participant_id\"] # not using redcap_event_name to allow broadcast of demographics vars\n",
    "tabular_df = pd.merge(pheno_var_df, baseline_demo_df, on=index_cols, how=\"left\")\n",
    "tabular_df[tabular_df[\"participant_id\"]==participants_with_follow_ups[0]]\n",
    "\n",
    "tabular_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df[~tabular_df[\"legacy_updrs3_scores\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\"diagnosis_date\", \"updrs_date\", \"legacy_updrs3_date\", \"moca_date\", \"MRI_date\", \"neuropsy_date\"]\n",
    "\n",
    "date_age_cols_dict = {}\n",
    "for col in date_cols:\n",
    "    date_age_cols_dict[col] = f\"{col.rsplit('_',1)[0]}_age\"\n",
    "\n",
    "age_cols = list(date_age_cols_dict.values())\n",
    "\n",
    "for date_col, age_col in date_age_cols_dict.items():\n",
    "    tabular_df = get_age_at_visit(tabular_df, date_col, age_col)\n",
    "\n",
    "tabular_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df[~tabular_df[\"legacy_updrs3_date\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save demo, mri_dates, and pheno (dx, updrs, moca, neuropsych) in separate files\n",
    "- remove DoB and other date columns \n",
    "- add age columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = [\"participant_id\", \"redcap_event_name\"]\n",
    "\n",
    "mri_cols = [\"session\", \"MRI_age\"]\n",
    "\n",
    "dx_cols = [\"diagnosis\", 'Hoehn and Yahr Stage: ', \"diagnosis_age\"]\n",
    "\n",
    "updrs_cols = ['Part I: Non-Motor Aspects of Experiences of Daily Living (nM-EDL)',\n",
    "              'Part II: Motor Aspects of Experiences of Daily Living (M-EDL)',\n",
    "              'Part III: Motor Examination', 'Part IV: Motor Complications', \n",
    "              'updrs_age',\n",
    "              'legacy_updrs3_scores', 'legacy_updrs3_age']\n",
    "\n",
    "# moca_cols = [\"moca_scores\", \"moca_age\"]\n",
    "moca_cols = [\n",
    "            \"TOTAL SCORE (make sure to include extra point for 12 years or less of education):    SCORE TOTAL (assurez-vous d'inclure un point supplémentaire pour 12 ans ou moins d'éducation) : \",\n",
    "            \"Did the participant receive +1 extra point for 12 years or less of education?    Le participant a-t-il reçu +1 point supplémentaire pour 12 ans ou moins d'études?\",\n",
    "            \"moca_age\"\n",
    "            ]\n",
    "\n",
    "neuropsych_cols = neuropsych_cols + [\"neuropsy_age\"]\n",
    "if \"neuropsy_date\" in neuropsych_cols:\n",
    "    neuropsych_cols.remove(\"neuropsy_date\")\n",
    "\n",
    "mri_df = tabular_df[index_cols + mri_cols]\n",
    "dx_df = tabular_df[index_cols + dx_cols].copy()\n",
    "updrs_df = tabular_df[index_cols + updrs_cols].copy()\n",
    "moca_df = tabular_df[index_cols + moca_cols].copy()\n",
    "neuropsych_df = tabular_df[index_cols + neuropsych_cols].copy()\n",
    "\n",
    "# filter na rows\n",
    "updrs_df = updrs_df.dropna(subset=updrs_cols, how='all')\n",
    "moca_df = moca_df.dropna(subset=moca_cols, how='all')\n",
    "\n",
    "# Save data to files\n",
    "mri_df.to_csv(mri_session_date_file, index=False)\n",
    "print(f\"Saved MRI session data to {mri_session_date_file}\")\n",
    "\n",
    "dx_df.to_csv(dx_file, index=False)\n",
    "print(f\"Saved diagnosis data to {dx_file}\")\n",
    "\n",
    "updrs_df.to_csv(updrs_file, index=False)\n",
    "print(f\"Saved UPDRS data to {updrs_file}\")\n",
    "\n",
    "moca_df.to_csv(moca_file, index=False)\n",
    "print(f\"Saved MoCA data to {moca_file}\")\n",
    "\n",
    "neuropsych_df.to_csv(neuropsych_file, index=False)\n",
    "print(f\"Saved neuropsych data to {neuropsych_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_df[tabular_df[\"redcap_event_name\"].isin([\"pre-redcap-baseline-1 (legacy)\"])][index_cols + moca_cols + updrs_cols + age_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save merged tabular data\n",
    "### TODO after finalizing format for the merged dataframe index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_cols  = [\"participant_id\", \"redcap_event_name\"]\n",
    "# save_cols = index_cols + dx_cols + updrs_cols + moca_cols + neuropsych_cols + age_cols\n",
    "# for col in date_cols:\n",
    "#     if col in save_cols:\n",
    "#         print(f\"removing {col}\")\n",
    "#         save_cols.remove(col)\n",
    "\n",
    "# n_save_cols = len(save_cols)    \n",
    "\n",
    "# print(f\"n_save_cols: {n_save_cols}\")\n",
    "\n",
    "# tabular_df.to_csv(tabular_file, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDRS legacy and redcap data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updrs3_2019_csv = f\"{data_release_dir}/tabular/recruitment/updrs3_2019.csv\"\n",
    "mds_updrs_hackathon_csv = f\"{data_release_dir}/tabular/recruitment/mds_updrs_hackathon.csv\"\n",
    "\n",
    "updrs3_2019_df = pd.read_csv(updrs3_2019_csv)\n",
    "mds_updrs_hackathon_df = pd.read_csv(mds_updrs_hackathon_csv)\n",
    "\n",
    "updrs3_2019_participants = updrs3_2019_df[\"Record ID:\"].unique()\n",
    "mds_updrs_hackathon_participants = mds_updrs_hackathon_df[\"Record ID:\"].unique()\n",
    "\n",
    "n_updrs3_2019_participants = len(updrs3_2019_participants)\n",
    "n_mds_updrs_hackathon_participants = len(mds_updrs_hackathon_participants)\n",
    "\n",
    "print(f\"Number of participants in UPDRS3 2019: {n_updrs3_2019_participants}\")\n",
    "print(f\"Number of participants in MDS UPDRS Hackathon: {n_mds_updrs_hackathon_participants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updrs_redcap_participants = pheno_var_df[pheno_var_df[\"Part III: Motor Examination\"].notna()][\"participant_id\"].unique()\n",
    "print(f\"Number of participants with UPDRS data in redcap: {len(updrs_redcap_participants)}\")\n",
    "\n",
    "paper_df = tabular_df.copy()\n",
    "paper_participants = paper_df[\"participant_id\"].unique()\n",
    "print(f\"Number of participants in paper: {len(paper_participants)}\")\n",
    "\n",
    "updrs_missing_participants = set(paper_participants) - set(updrs_redcap_participants) - set(updrs3_2019_participants) - set(mds_updrs_hackathon_participants)\n",
    "n_updrs_missing_participants = len(updrs_missing_participants)\n",
    "print(f\"Number of participants missing UPDRS data: {n_updrs_missing_participants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(paper_participants) & set(mds_updrs_hackathon_participants)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suivi updrs dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_recruit_manifest_xls = f\"{data_release_dir}/tabular/recruitment/Suivi_RPQ.xlsx\"\n",
    "\n",
    "col_range = \"A:BD\"\n",
    "\n",
    "col_rename_dict = {\n",
    "    \"subj_id\":\"participant_id\",\n",
    "    \"IRM01\\n(J-M-A)\":\"IRM01_date\", \"#IRM 1\\n PD\":\"IRM01_PD\", \"#IRM 1\\n CTRL\":\"IRM01_CTRL\", \n",
    "    \"# IRM 1\\n RBD\":\"IRM01_RBD\", \"# IRM 1\\nOTHER\":\"IRM01_OTHER\",\n",
    "    \"IRM 2 \\n(J-M-A)\":\"IRM02_date\", \"#IRM 2\\n PD\":\"IRM02_PD\", \"#IRM 2\\n CTRL\":\"IRM02_CTRL\", \n",
    "    \"# IRM 2\\n RBD\":\"IRM02_RBD\", \"# IRM 2 OTHER\":\"IRM02_OTHER\",\n",
    "    \"IRM 3\\n(J-M-A)\":\"IRM03_date\", \"#IRM 3\\n PD\":\"IRM03_PD\", \"#IRM 3\\n CTRL\":\"IRM03_CTRL\", \n",
    "    \"# IRM 3\\n RBD\":\"IRM03_RBD\", \"# IRM 3 OTHER\":\"IRM03_OTHER\",\n",
    "    \"MDS-UPDRS_partie_III_1\\n(J-M-A)\":\"MDS-UPDRS_III_v01_date\", \n",
    "    \"MDS-UPDRS_complet (J-M-A)\": \"MDS-UPDRS_complete_date\"\n",
    "    }\n",
    "\n",
    "useful_cols = col_rename_dict.values()\n",
    "\n",
    "suivi_df = pd.read_excel(current_recruit_manifest_xls,sheet_name=\"En cours\", engine='openpyxl', usecols=col_range)\n",
    "suivi_df = suivi_df.rename(columns=col_rename_dict)[useful_cols].copy()\n",
    "\n",
    "# remove the row with tally\n",
    "suivi_df = suivi_df.drop([0])\n",
    "\n",
    "# remove rows without participant_id\n",
    "suivi_df = suivi_df.dropna(axis=0, subset=[\"participant_id\"])\n",
    "suivi_df = suivi_df[~suivi_df[\"participant_id\"].astype(str).isin([\"0\"])] \n",
    "suivi_df[\"participant_id\"] = suivi_df[\"participant_id\"].str.strip().astype(str)\n",
    "\n",
    "# remove subjects without imaging data\n",
    "suivi_df = suivi_df[(suivi_df[\"IRM01_PD\"] == 1) | (suivi_df[\"IRM01_CTRL\"] == 1) | \n",
    "                    (suivi_df[\"IRM01_RBD\"] == 1) | (suivi_df[\"IRM01_OTHER\"] == 1) |\n",
    "                    (suivi_df[\"IRM02_PD\"] == 1) | (suivi_df[\"IRM02_CTRL\"] == 1) | \n",
    "                    (suivi_df[\"IRM02_RBD\"] == 1) |(suivi_df[\"IRM02_OTHER\"] == 1) |\n",
    "                    (suivi_df[\"IRM03_PD\"] == 1) | (suivi_df[\"IRM03_CTRL\"] == 1) | \n",
    "                    (suivi_df[\"IRM03_RBD\"] == 1) |(suivi_df[\"IRM03_OTHER\"] == 1) ]\n",
    "\n",
    "\n",
    "# fix participant_id formatting issues\n",
    "# Some rows have Dx in participant_id and one participant with two IDs with \"=\"\n",
    "possible_delimiters = [\" \", \"(\", \"=\", \"\\n\"]\n",
    "for delim in possible_delimiters:        \n",
    "    suivi_df[\"participant_id\"] = suivi_df[\"participant_id\"].str.strip().str.split(pat=delim, n=1, expand=True)[0]\n",
    "\n",
    "# nipoppy_participants_current\n",
    "nipoppy_participants_current = suivi_df[\"participant_id\"].dropna().unique()\n",
    "\n",
    "suivi_df[\"MDS-UPDRS_III_v01_date\"] = pd.to_datetime(suivi_df[\"MDS-UPDRS_III_v01_date\"], errors=\"coerce\")\n",
    "suivi_df[\"MDS-UPDRS_complete_date\"] = pd.to_datetime(suivi_df[\"MDS-UPDRS_complete_date\"], errors=\"coerce\")\n",
    "\n",
    "suivi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suivi_updrs_participants = suivi_df[(suivi_df[\"MDS-UPDRS_III_v01_date\"].notna() | (suivi_df[\"MDS-UPDRS_complete_date\"].notna()))][\"participant_id\"]\n",
    "n_suivi_updrs_participants = len(suivi_updrs_participants)\n",
    "print(f\"Number of participants with UPDRS data in suivi: {n_suivi_updrs_participants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(suivi_updrs_participants) & set(paper_participants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(suivi_updrs_participants) & set(updrs_missing_participants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_updrs_participants_df = pd.DataFrame(list(updrs_missing_participants), columns=[\"participant_id\"])\n",
    "missing_updrs_participants_df[\"Suivi_date_present\"] = \"no\"\n",
    "missing_updrs_participants_df.loc[missing_updrs_participants_df[\"participant_id\"].isin(suivi_updrs_participants),\"Suivi_date_present\"] = \"yes\"\n",
    "missing_updrs_participants_df = pd.merge(missing_updrs_participants_df, demo_var_df, on=\"participant_id\", how=\"left\")\n",
    "missing_updrs_participants_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_updrs_participants_df_PD = missing_updrs_participants_df[\n",
    "                                                                # (missing_updrs_participants_df[\"Suivi_date_present\"]==\"yes\") & \n",
    "                                                                (missing_updrs_participants_df[\"group\"]==\"PD\") &\n",
    "                                                                (missing_updrs_participants_df[\"redcap_event_name\"]==\"Baseline (Arm 1: C-OPN)\")\n",
    "                                                                ]\n",
    "len(missing_updrs_participants_df_PD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_updrs_participants_df_PD.reset_index().drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_updrs_participants_df_PD.to_csv(f\"{data_release_dir}/tabular/recruitment/missing_updrs_participants_after_hackathon.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nipoppy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
